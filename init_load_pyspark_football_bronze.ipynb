{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad828a46-53d1-4e08-bbb8-ff1d1bb0e9ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**This notebook demonstrates how to load data files using PySpark in a loop. It is intended as a reference for efficiently reading multiple files from a filestore and processing them with PySpark.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9975aedd-501d-42ad-b6cc-d28746036998",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create bronze layer database"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS learning_catalog.football_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bc8d3d-e0ce-4906-a488-e5ed0f425292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4:** Loop through files in filestore  \n",
    "In this step, we iterate over all files located in the specified filestore directory. For each file, we load its contents using PySpark, allowing us to process multiple files efficiently in a single workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a600bd4-2e62-4e2f-898e-8996c0a60e72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loop through files in landing folder"
    }
   },
   "outputs": [],
   "source": [
    "import os # Import os module\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# Define the base paths\n",
    "input_path = \"/Volumes/learning_catalog/raw_data/landing/\"\n",
    "bronze_schema_name = \"learning_catalog.football_bronze\"\n",
    "\n",
    "# Get a list of all files in the input directory.\n",
    "file_list = dbutils.fs.ls(input_path) # Returns a list of FileStatus objects\n",
    "\n",
    "# Loop through each file in list\n",
    "\n",
    "for file_info in file_list:\n",
    "    filename = os.path.basename(file_info.path)\n",
    "\n",
    "    if filename.lower().endswith(\".csv\"):  # Check if the file is a CSV file\n",
    "        table_name_suffix = filename.replace(\".csv\", \"\")\n",
    "\n",
    "        # Next construct file paths and table name\n",
    "        full_input_path = file_info.path\n",
    "        full_table_name = f\"{bronze_schema_name}.raw_{table_name_suffix}\"\n",
    "        \n",
    "        print(f\"Processing file: {full_input_path} -> table: {full_table_name}\")\n",
    "\n",
    "    # Begin try catch to process and load with PySpark\n",
    "    # Read\n",
    "    try:\n",
    "        # Read\n",
    "        # In production would define schema with StuctType to avoid errors, but for test system just infer\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .load(full_input_path)\n",
    "\n",
    "        # Check if the '_corrupt_record' column exists before trying to use it\n",
    "        if \"_corrupt_record\" in df.columns:\n",
    "            print(f\"Corrupt records found in {filename}! Inspecting...\")\n",
    "            df.filter(\"_corrupt_record IS NOT NULL\").show()\n",
    "\n",
    "        else:\n",
    "            # If no corrupt records, proceed\n",
    "            print(f\"No corrupt records found in {filename}.\")\n",
    "\n",
    "\n",
    "        # Transform and add cols for best practice\n",
    "        df_with_metadata = df.withColumn(\"loaded_timestamp\", current_timestamp()) \\\n",
    "            .withColumn(\"source_file\", lit(filename)) # Write str var into every row\n",
    "\n",
    "        # Write\n",
    "        df_with_metadata.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "\n",
    "        print(f\"Successfully created Bronze table '{full_table_name}'\")\n",
    "\n",
    "    except Exception as e:   \n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "print(\"\\nIngestion process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3e41361a-b636-4783-8e57-28a7210c2c66",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query catalog and schema with SQL"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN learning_catalog.football_bronze;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3401731505016517,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "init_load_pyspark_football_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
